{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Part 1\n",
    "\n",
    "Assume there is an unknown function $F^*$ that maps $X_n$'s to $Y$'s such that $Y=F^*(X)$. \n",
    "\n",
    "Let's call $F^*$ our *Target* function.\n",
    "\n",
    "We generally can't hope to ever observe $F^*$ directly, but we have examples (or samples) of its inputs and its output.\n",
    "\n",
    "![](./images/some_data_annotated.png)\n",
    "\n",
    "The goal of a **Supervised Learning** algorithm is to find a good *Model* $\\hat{F}$ of the unknown *Target* $F^*$ based on these examples.\n",
    "\n",
    "\n",
    "## How do algorithms search for models?\n",
    "\n",
    "\n",
    "Algorithms **learn** from data by computing an *estimate* $\\hat{F}$ from the data examples. This can be thought of as *fitting* a function to the data examples. Mathematically, this amounts to solving an optimization problem of the following form:\n",
    "\n",
    "$\\hat{F(X)} = \\underset{\\theta}{\\operatorname{argmin}} L(Y,f(X;\\theta))$\n",
    "\n",
    "\n",
    "Where $L(Y,f(X;\\theta))$ is a function that measures the error made by some function $f(X;\\theta)$ in approximating $Y$. In other words, algorithms will try to find optimal values for a set of parameters $\\theta$ such that the error in using $f$ evaluated at example inputs $X$ to approximate example outputs $Y$ is minimized. This process is called **Training** a model and the examples used to train the model are called the **Training Set**. \n",
    "\n",
    "There are many options of error measure $L$. The choice of $L$ depends on the type of problem (or type of Task) and the choice of algorithm. $L$ is called the **Loss** function, and it's also sometimes called the **Energy** or **Cost** function.\n",
    "\n",
    "Different algorithms will focus on different classes of function $f$ with a different number of parameters $\\theta$ to approximate $Y$.\n",
    "\n",
    "Here is a visual representation of these ideas.\n",
    "\n",
    "## Imagine a space inhabited by functions...\n",
    "\n",
    "![](./images/model_space.gif)\n",
    "\n",
    "$\\hat{F_1 (x)}$ and $\\hat{F_2 (x)}$ are a models of $F^*(x)$. Both are close to the real thing, but are still not exactly it. There is an idea of **error**.\n",
    "\n",
    "A good estimate of $F^*(x)$, given a dataset, is one that **minimizes** this error\n",
    "\n",
    "\n",
    "Different algorithms will be most effective at finding models within different parts of this space. Some algorithms are more restrictive and are ONLY able to search in very limited parts of the space.\n",
    "\n",
    "![](./images/function_space.png)\n",
    "\n",
    "**DISCLAIMER**: This diagram is merely illustrative and does not reflect the real overlaps and boundaries of different classes of functions.\n",
    "\n",
    "## How do we measure error?\n",
    "\n",
    "An error measure is a quantity that denotes how close two functions are to one another. A straightfoward way of measuring this distance between functions, is to look at how different their Y coordinates are for the same X:\n",
    "\n",
    "![](./images/Error1.png)\n",
    "\n",
    "Now we need to turn these point-wise distances into a single measure that representes the distance between two whole functions. One idea would be to simply take the **average** of the point-wise distances\n",
    "\n",
    "![](./images/Error2.png)\n",
    "\n",
    "Notice however that sometimes the point-wise distance is positive ($F*$ is *above* than $\\hat{F}$), and sometimes it is negative ($F*$ is *below* than $\\hat{F}$). This can be very misleading, as by summing negative numbers and positive numbers together, one might get a measure of the total distance that is quite small, even exactly **zero**, when the two functions are clearly not equal to each other for all X's.\n",
    "\n",
    "To solve this problem, we would like a meaure that avoids summing positive and negative numbers together. We would like something that make the point-wise distances always be positive, not matter what function is above or below the other for a given X. Here, we will use the average of the *squared* point-wise distances, or the **mean squared error**:\n",
    "\n",
    "![](./images/Error3.png)\n",
    "\n",
    "There are many other valid error measures that are useful in different scenarios in Machine Learning. We will see some of them later in this workshop, but for now, let's direct our attention to the last missing piece of the puzzle...\n",
    "\n",
    "## How do we minimize the error measure?\n",
    "\n",
    "\n",
    "The error measure is itself a function $L(Y,f(X;\\theta))$ called the **Loss Function**. It takes in a function $f(X)$ and its parameters $\\theta$ and outputs the error we incurr in when using this $f(X;\\theta)$ to approximate $Y$. Remember that here $X$ and $Y$ are the examples/observed values in your dataset.\n",
    "\n",
    "When we say we want to minimize the error, what we are really saying is we want to find what values of the parameters $\\theta$, when plugged into the function $f$ evaluated at our data $X$, get us closest to our data $Y$.\n",
    "\n",
    "In other words, we want to find *critical points* of the function $L$ with respect to $\\theta$.\n",
    "\n",
    "Sometimes, depending on the choice of function $f(X;\\theta)$, it is possible to solve this problem analytically by taking the derivative of $L(Y,f(X;\\theta))$ with respect to $\\theta$ and setting it to zero. However, for many choices of $f(X;\\theta)$, calculating the derivative by hand is impractical, so we use **numerical methods** instead.\n",
    "\n",
    "In general, these numerical methods consist in choosing an initial value for the parameters $\\theta$, and then slightly changing them in a direction that hopefully descreases the error. Then rinse and repeat a large number of times until the method converges to a critical point, or we get close enough to one such that we are satisfied with the results.\n",
    "\n",
    "Visually, here is what we get when we plot different values of $\\theta$ against the error in using a linear function with just one parameter $f(X)=\\theta X$ to approximate $Y=2 X$. If we start with $\\theta=0$, the error goes down as we move $\\theta$ closer to the true value, hits zero when $\\theta=2$ and starts increasing when we move the value away from the true value:\n",
    "\n",
    "![](./images/convex.png)\n",
    "\n",
    "## Also, about that first assumption we made...\n",
    "\n",
    "We started this section by assuming there was an unknown function $F^*$ that maps $X_n$'s to $Y$'s such that $Y=F^*(X)$, remember that?\n",
    "\n",
    "But in real life, things are messy. Whether you are taking measures in a physical experiment, observing animal behaviour, or whatever your scientific data collection edeavour may be, you will rarely get data where this assumption holds exactly.\n",
    "\n",
    "In other words, for Machine Learning to be useful in real life, we have to loosen that assumption a little bit. We will instead assume the following:\n",
    "\n",
    "There is an unknown fuction $F^*$ that **approximately** maps $X_n$'s to $Y$'s such that $Y=F^*(X) + \\epsilon$. Where $\\epsilon$ is a random variable that follows some unknown probability distribution.\n",
    "\n",
    "All this means is our Y's will fluctuate around some function $F^*(X)$ instead of following it exactly. \n",
    "\n",
    "That is, instead of seeing data like this:\n",
    "\n",
    "![](./images/fofx_exact.png)\n",
    "\n",
    "You will (much) more often than not see data like this:\n",
    "\n",
    "![](./images/fofx_noisy.png)\n",
    "\n",
    "\n",
    "Now let's see how these ideas work in practice.\n",
    "\n",
    "First, let's pick an arbitrary function of a single variabe $x$ to use as our target function, say, $F^*(x)=\\cos(\\frac{3}{2}\\pi x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = lambda x: np.cos(1.5 * np.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate 50 data samples consisting of pairs of values: a random value for $x$ between 0 and 1, and $F^*(x)$ plus random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [(x,f(x) + np.random.normal(0,1) * 0.1) for x in np.sort(np.random.uniform(0,1,50))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot both the target function and the samples we've generated to see what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = zip(*samples) \n",
    "\n",
    "X_test = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.plot(X_test,f(X_test),color='r',label='Target')\n",
    "\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we didn't know what the target function is. Let's try out different machine learning algorithms and see if we can figure it out using only our samples (blue dots).\n",
    "\n",
    "First, let's try out an algorithm called Linear Regression. Using this algorithm means we will restrict our search for models to the space of functions of the form $Y = ax + b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X = np.array(X).reshape(-1,1) # Scikit-learn needs this to work with only one input/feature\n",
    "\n",
    "linear_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"Linear Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", linear_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green line looks nothing like the red line from before, as expected since we know the target function is not a straight line. Since our data displays a \"curvy\" pattern, we should try an algorithm that searches in part of the function space that includes curvy functions.\n",
    "\n",
    "Let's try Linear Regression again, but this time we'll widen our search space to include functions of the form $Y=ax^2 + bx + c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LINEAR REGRESSION WITH POLYNOMIAL BASIS EXPANSIONS\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2) # Set the Degree of the polynomial to 2\n",
    "\n",
    "square_model = LinearRegression()\n",
    "\n",
    "X_poly = poly.fit_transform(X)  #Transform X into [X , X^2]\n",
    "\n",
    "square_model.fit(X_poly,Y)\n",
    "\n",
    "X_poly_test = poly.fit_transform(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, square_model.predict(X_poly_test), color='g',label='Square Fit')\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", square_model.score(X_poly,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better!\n",
    "\n",
    "However, the shape of the curve is still not quite right. Let's try an algorithm that widens the search space even more to include pretty much any kind of curvy function imaginable: Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S TRY A NEURAL NETWORK\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100,50,25,10), max_iter=2000) #Neural Networks are flexible, Try different numbers of layers and neurons....\n",
    "\n",
    "mlp_model.fit(X,Y)\n",
    "\n",
    "Y_hat = mlp_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label='Target')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"MLP\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", mlp_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you know if your Model is any good?\n",
    "\n",
    "In the first example above, the model was obviously bad... the model is not even trying to (because it can't) catch the \"curvy\" pattern in the data. This phenomenon is called **Underfitting**.  \n",
    "\n",
    "In the second example, the model looks close!\n",
    "\n",
    "In the third example you have to try different parameters, but you can get very, very close to the target!\n",
    "\n",
    "What we did above was, borrowing from statistical jargon, training models by minimizing the error on **in-sample** data... but what we are actually interested in is how our model performs on **out-of-sample** data! We care about predicting $Y$ in the population!\n",
    "\n",
    "So are these good models of $F^*$? \n",
    "\n",
    "The answer depends on what you are trying to predict. If you are concerned with predicting only points inside the $[0,1]$ interval, then yes, these models seem approximate well points that were not included in the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, square_model.predict(X_poly_test), color='g',label='Square Fit')\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", square_model.score(X_poly_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label='Target')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"MLP\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", mlp_model.score(X_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But here's what our models looks like when we extend the range of the plot a bit to show points farther away from the sample we used to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.linspace(-1,2,100).reshape(-1,1)\n",
    "\n",
    "new_Y = f(new_X)\n",
    "\n",
    "new_X_poly = poly.fit_transform(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_X,new_Y,label=\"Target\")\n",
    "\n",
    "plt.plot(new_X, linear_model.predict(new_X), color='r',label=\"Linear Fit\")\n",
    "plt.plot(new_X, square_model.predict(new_X_poly), color='g',label=\"Square Fit\")\n",
    "plt.plot(new_X, mlp_model.predict(new_X), color='y',label=\"MLP Fit\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the in-sample error, or **Training Error**, is not a very good measure of how well the model will do on out-of-sample data. In fact, models can have an arbitrarily small **Training Error** and be very bad at predicting out-of-sample values. To see this, consider what would happen if you were to try the example above with a very wiggly function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LET'S TRY FITTING A DEGREE 100 POLYNOMIAL TO OUR SAMPLES\n",
    "\n",
    "poly_too_high = PolynomialFeatures(degree=100)\n",
    "\n",
    "too_high_model = LinearRegression()\n",
    "\n",
    "X_poly = poly_too_high.fit_transform(X)  #TRANSFORM X INTO X^n\n",
    "\n",
    "too_high_model.fit(X_poly,Y)\n",
    "\n",
    "X_poly_test = poly_too_high.fit_transform(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label=\"Samples\")\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, too_high_model.predict(X_poly_test), color='g',label=\"Degree 100 Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", too_high_model.score(X_poly,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label=\"Samples\")\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, too_high_model.predict(X_poly_test), color='g',label=\"Degree 100 Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", too_high_model.score(X_poly_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND THEN ZOOMING OUT\n",
    "\n",
    "new_X_too_high = poly_too_high.fit_transform(new_X)\n",
    "\n",
    "plt.scatter(new_X,new_Y,label=\"Target\")\n",
    "plt.plot(new_X, too_high_model.predict(new_X_too_high), color='r',label=\"Linear Fit\")\n",
    "plt.ylim((-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ***training*** error here is low - the model actually passes right through most of the points in the sample! But it is also obvious that this model is not very accurate at predicting points outside the sample, whether that's close to the range of the sample or not. \n",
    "\n",
    "So in practice you will need to have not one, but TWO datasets. One to train your model on and another to validate it. This second dataset is called a **Test Set** and it is a collection of inputs and outputs you obtained from the same source as your training data, but that you **did not use to train your model.**\n",
    "\n",
    "It is the error in this dataset, called the **Test Error** that matters when we talk about the quality of our predictions, and hence the quality of our model.\n",
    "\n",
    "Concretely, you will train your model on the **Training Set**, looking to minimize the **Training Error**. Then you will pick up the model you trained and plug in the inputs from your **Test Set**. You will then use the outputs you get from your model and compare them to the outputs in your **Test Set**. This will allow you to compute your **Test Error**.\n",
    "\n",
    "Rinse and repeat until you're statisfied with the performance of your model on the **Test Set**!\n",
    "\n",
    "A common rule-of-thumb is to break your initial data set in two chunks: about 80% of all examples go into your **Training Set**, the remaining 20% are set aside for your **Test Set**.\n",
    "\n",
    "Another popular approach is to break your initial data set in THREE chunks: a **Training Set**, a **Validation Set** and a **Test Set**, where the **Validation Set** is not used directly to train the model, but the error in this set is used as the yardstick to *fine-tune* a model, so it participates indirectly in the training.\n",
    "\n",
    "\n",
    "Let's look into some examples with real datasets to see how this works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example 1: Flower Species Classification\n",
    "\n",
    "In the toy examples we've seen above, we used a mathematical function to generate points and used different algorithms to try and approximate it. In that case, both our input X and output Y were quantitative (a real number representing a quantity). In Machine Learning this type of task, i.e. predicting a quantitative output, is called **Regression**.\n",
    "\n",
    "Now let's look at an example where the output is no longer quantitative, but *categorical*, meaning that the output variable represents categories of things - a task called **Classification**.\n",
    "\n",
    "We will use a popular Python Machine Learning library called **scikit-learn** to show how it works in practice.\n",
    "\n",
    "### The Iris dataset\n",
    "\n",
    "This example uses a classic dataset called the \"Iris\" dataset. It contains a number of measurements of different species of Iris flowers along with a label indicating which of 3 species of Iris the measurements came from.\n",
    "\n",
    "We will train a model to take in measurements as inputs and predict the species.\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "headers = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']\n",
    "\n",
    "iris_dataset = read_csv('./data/iris.csv', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset) # A GOOD FIRST STEP IS TO LOOK AT THE ACTUAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.describe()) # THEN COMPUTE SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.groupby('species').size()) # ARE THERE IMBALANCES IN THE OUTPUT CATEGORIES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(iris_dataset) # PLOT ALL VARIABLES 2 BY 2... ARE THERE ANY VISIBLE PATTERNS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S CREATE A TRAINING SET AND A TEST SET\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris_dataset.values[:,0:4]\n",
    "Y = iris_dataset.values[:,4]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S TRY DIFFERENT ALGORITHMS - FIRST A LINEAR MODEL: LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "lr_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = lr_model.predict(X_test)\n",
    "\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_hat, Y_test)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER MODEL: TREE CLASSIFIERS\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "tree_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = tree_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(Y_hat, Y_test)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE MORE: SUPPORT VECTOR MACHINES (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel = 'linear')\n",
    "\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = svm_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(Y_hat, Y_test)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more options of Algorithms, see: https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Wine Classification\n",
    "\n",
    "Now it's your turn. This next dataset contains a number of measurements done in a chemical analysis of 3 different types of wine.\n",
    "\n",
    "You will train a Support Vector Machine model to predict the type of wine based on the measurements. Use the example above as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "headers = ['wine_type','alcohol', 'malic_acid','ash','alcalinity_of_ash','magnesium',\n",
    "           'total_phenols','flavanoids','nonflavanoid_phenols','proanthocyanins','color_intensity','hue','OD280_OD315','proline']\n",
    "\n",
    "wine_dataset = read_csv('./data/wine.csv', names = headers)\n",
    "\n",
    "# PRINT SUMARY STATISTICS\n",
    "\n",
    "# PRINT THE DATASET\n",
    "\n",
    "# LOOK FOR IMBALANCES - THE OUPUT VARIABLE IS 'wine_type'\n",
    "\n",
    "# LOOK FOR VISUAL PATTERNS\n",
    "\n",
    "# CREATE TRAINING AND TEST SETS - THE OUTPUT VARIBALE 'wine_type' IS ON COLUMN 0\n",
    "\n",
    "# FIT A MODEL TO THE TRAINING SET\n",
    "\n",
    "# CHECK ITS PERFORMANCE ON THE TEST SET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example 2: Image Classification\n",
    "\n",
    "In the Flower Species Classification example we had a dataset with numerical features/inputs and a categorical response/output. The categories in our example appeared as text, but the code transformed them into numbers under the hood. What other types of data can be encoded as numbers?\n",
    "\n",
    "The answer is: pretty much anything can. \n",
    "\n",
    "Let's look at a Classification problem using images. We will train a model to classify images as either Apples or Oranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "an_apple = Image.open('./Fruit-Images-Dataset/Training/apples/0_100.jpg')\n",
    "\n",
    "print(\"This is how the computer 'sees' the image: \\n\")\n",
    "\n",
    "print(np.array(an_apple)[:,:,0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(an_apple)[:,:,:]) # TRY PRINTING THE THREE DIFFERENT CHANNELS - [:,:,0], [:,:,1], [:,:,2]\n",
    "print(\"This is how we see it:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_orange = Image.open('./Fruit-Images-Dataset/Training/oranges/0_100.jpg')\n",
    "\n",
    "plt.imshow(an_orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S CREATE OUR TRAINING SET!\n",
    "\n",
    "apples_dir = './Fruit-Images-Dataset/Training/apples/'\n",
    "oranges_dir = './Fruit-Images-Dataset/Training/oranges/'\n",
    "\n",
    "#os.listdir(apples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD APPLES STEP BY STEP - WITH ILLUSTRATIVE VARIABLE NAMES\n",
    "\n",
    "apples = []\n",
    "\n",
    "for filename in os.listdir(apples_dir):\n",
    "    \n",
    "    apple_full_path = apples_dir + filename\n",
    "    \n",
    "    apple = Image.open(apple_full_path)\n",
    "    \n",
    "    apple_flattened = np.array(apple).flatten()\n",
    "    \n",
    "    apple_resized = apple_flattened/255\n",
    "    \n",
    "    a_training_example = (apple_resized, \"apple\")\n",
    "    \n",
    "    apples.append(a_training_example)\n",
    "\n",
    "apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ORANGES WITH A PYTHON ONE-LINER... CAN YOU SEE HOW THIS DOES THE SAME THING AS THE FOR-LOOP BLOCK ABOVE?\n",
    "\n",
    "oranges = [ (np.array(Image.open(oranges_dir + img)).flatten()/255,\"orange\") for img in os.listdir(oranges_dir) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP SEPARATES ARRAYS FROM LABELS\n",
    "\n",
    "X_train,Y_train = zip(*(apples + oranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE'S HOW ZIP WORKS:\n",
    "\n",
    "my_list = [(np.array([1,2,3]), \"one\"),(np.array([4,5,6]), \"two\")]\n",
    "\n",
    "a,b = zip(*my_list)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACK \"STACKS\" VECTORS INTO A MATRIX\n",
    "\n",
    "X_train = np.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE'S HOW STACK WORKS\n",
    "\n",
    "print(a)\n",
    "\n",
    "np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW THE TEST SET\n",
    "\n",
    "apples_dir = './Fruit-Images-Dataset/Test/apples/'\n",
    "oranges_dir = './Fruit-Images-Dataset/Test/oranges/'\n",
    "\n",
    "apples = [ (np.array(Image.open(apples_dir + img)).flatten()/255,\"apple\") for img in os.listdir(apples_dir) ]\n",
    "\n",
    "oranges = [ (np.array(Image.open(oranges_dir + img)).flatten()/255,\"orange\") for img in os.listdir(oranges_dir) ]\n",
    "\n",
    "X_test,Y_test = zip(*(apples + oranges))\n",
    "\n",
    "X_test = np.stack(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THEN TRAIN A LOGISTIC REGRESSION MODEL AND SEE HOW IT DOES\n",
    "\n",
    "lr_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = lr_model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(Y_hat, Y_test)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Handwritten Digit Classification\n",
    "\n",
    "Your turn again. This next dataset is another classic: the MNIST dataset of handwritten digits.\n",
    "\n",
    "In this exercise, you will train a model of your choice on images of handwritten numbers that sometimes look alike when people write them: 0, 6 and 8.\n",
    "\n",
    "Use all the examples we've seen so far as insipration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START BY CREATING YOUR TRAINING SET...\n",
    "\n",
    "zero_dir = './MNIST-Dataset/Training/0/'\n",
    "six_dir = './MNIST-Dataset/Training/6/'\n",
    "eight_dir = './MNIST-Dataset/Training/8/'\n",
    "\n",
    "\n",
    "X_train,Y_train = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...AND YOUR TEST SET\n",
    "\n",
    "zero_dir = './MNIST-Dataset/Test/0/'\n",
    "six_dir = './MNIST-Dataset/Test/6/'\n",
    "eight_dir = './MNIST-Dataset/Test/8/'\n",
    "\n",
    "X_test, Y_test = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN YOUR MODEL AND EVALUATE ITS PERFORMANCE ON THE TEST SET\n",
    "\n",
    "my_model = ####\n",
    "\n",
    "Y_hat = my_model(####)\n",
    "\n",
    "plot_confusion_matrix(my_model,X_test, Y_test)\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing what we've seen so far\n",
    "\n",
    "Based on what we've covered so far, a Machine Learning program will generally have the following elements:\n",
    "\n",
    "1. A class of functions $f(x;\\theta)$ that the algorithm will use to try and approximate the taget $F^*$. \n",
    "\n",
    "    a. $f(x;\\theta)$ can be a restrictive class of functions such as linear functions ($\\beta_0 + \\sum{\\beta_i X_i}$) or a very flexible one, such as Neural Networks (we will see what they look like on the next notebook)\n",
    "    \n",
    "2. A Training Set: a dataset containing examples of inputs and outputs of interest.  \n",
    "\n",
    "3. A Test Set: another dataset with examples of inputs and outputs that are not used to train the model.\n",
    "\n",
    "4. A measure of the error in using $f(x;\\theta)$ to approximate $F^*$, called the Loss function $L(Y,f(X;\\theta))$. $L$ is used to train the model on the training set (see point 5) and can be used to measure performance on the Test Set. \n",
    "\n",
    "5. A way of solving the optimization problem: $\\hat{F(X)} = \\underset{\\theta}{\\operatorname{argmin}} L(Y,f(X;\\theta))$\n",
    "\n",
    "In the **scikit-learn** examples above, you will notice that 4, 5 and certain aspects of 1 are done mostly under the hood, leaving very little control up to you.\n",
    "\n",
    "Next we turn to a more modern Machine Learning library that is better suited for high performance and solving difficult problems: **PyTorch**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
