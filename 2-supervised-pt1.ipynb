{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning - Part 1\n",
    "\n",
    "Assume there is an unknown function $F^*$ that maps $X_n$'s to $Y$'s such that $Y=F^*(X)$. \n",
    "\n",
    "Let's call $F^*$ our *Target* function.\n",
    "\n",
    "We generally can't hope to ever observe $F^*$ directly, but we have examples (or samples) of its inputs and its output.\n",
    "\n",
    "![](./images/some_data_annotated.png)\n",
    "\n",
    "The goal of a **Supervised Learning** algorithm is to find a good *Model* $\\hat{F}$ of the unknown *Target* $F^*$ based on these examples.\n",
    "\n",
    "\n",
    "## How do algorithms search for Models?\n",
    "\n",
    "\n",
    "Algorithms **learn** from data by computing an *estimate* $\\hat{F}$ from the data examples. This can be thought of as *fitting* a function to the data examples:\n",
    "\n",
    "$\\hat{F(X)} = \\underset{\\theta}{\\operatorname{argmin}} L(Y,f(X;\\theta))$\n",
    "\n",
    "\n",
    "Where $L(Y,f(X;\\theta))$ is a function that measures the error made by some function $f(X;\\theta)$ in approximating $Y$. In other words, algorithms will try to find optimal values for a set of parameters $\\theta$ such that the error in using $f$ evaluated at example inputs $X$ to approximate example outputs $Y$ is minimized. This process is called **Training** a model and the examples used to train the model are called the **Training Set**. \n",
    "\n",
    "There are many options of error measure $L$. The choice of $L$ depends on the type of problem (or type of Task) and the choice of algorithm. $L$ is called the **Loss** function, and it's also sometimes called the **Energy** or **Cost** function. We'll come back to this subject later in the workshop.\n",
    "\n",
    "Different algorithms will focus on different classes of function $f$ with a different number of parameters $\\theta$ to approximate $Y$.\n",
    "\n",
    "Here is a visual representation of these ideas.\n",
    "\n",
    "## Imagine a space inhabited by functions...\n",
    "\n",
    "![](./images/model_space.gif)\n",
    "\n",
    "$\\hat{F_1 (x)}$ and $\\hat{F_2 (x)}$ are a models of $F^*(x)$. Both are close to the real thing, but are still not exactly it. There is an idea of **error**.\n",
    "\n",
    "Different algorithms will be most effective at finding models within different parts of this space. Some algorithms are more restrictive and are ONLY able to search in very limited parts of the space.\n",
    "\n",
    "![](./images/function_space.png)\n",
    "\n",
    "**DISCLAIMER**: This diagram is merely illustrative and does not reflect the real overlaps and boundaries of different classes of functions.\n",
    "\n",
    "\n",
    "Here is how you can see these ideas in practice:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "f = lambda x: np.cos(1.5 * np.pi * x)\n",
    "\n",
    "samples = [(x,f(x) + np.random.normal(0,1) * 0.1) for x in np.sort(np.random.uniform(0,1,50))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = zip(*samples) \n",
    "\n",
    "X_test = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.plot(X_test,f(X_test),color='r',label='Target')\n",
    "\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IS LINEAR REGRESSION USEFUL ON THIS DATASET?\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X = np.array(X).reshape(-1,1) # Scikit-learn needs this to work with only one input/feature\n",
    "\n",
    "linear_model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = linear_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"Linear Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", linear_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S TRY WITH POLYNOMIAL BASIS EXPANSIONS\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "square_model = LinearRegression()\n",
    "\n",
    "X_poly = poly.fit_transform(X)  #TRANSFORM X INTO X^n\n",
    "\n",
    "square_model.fit(X_poly,Y)\n",
    "\n",
    "X_poly_test = poly.fit_transform(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, square_model.predict(X_poly_test), color='g',label='Square Fit')\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", square_model.score(X_poly,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S TRY A NEURAL NETWORK\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(100,50,25,10), max_iter=2000) #NEURAL NETWORKS ARE FLEXIBLE, TRY DIFFERENT NUMBERS OF LAYERS AND NEURONS...\n",
    "\n",
    "mlp_model.fit(X,Y)\n",
    "\n",
    "Y_hat = mlp_model.predict(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label='Target')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"MLP\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", mlp_model.score(X,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do you know if your Model is any good?\n",
    "\n",
    "In the first example above, the model was obviously bad... the model is not even trying to (because it can't) catch the \"curvy\" pattern in the data. This phenomenon is called **Underfitting**.  \n",
    "\n",
    "In the second example, the model looks close!\n",
    "\n",
    "In the third example you have to try different parameters, but you can get very, very close to the target!\n",
    "\n",
    "What we did above was, borrowing from statistical jargon, **training models** by minimizing the error on *in-sample* data... but what we are actually interested in is how our model performs on *out-of-sample* data! We care about predicting $Y$ in the population!\n",
    "\n",
    "So are these good models of $F^*$? \n",
    "\n",
    "The answer depends on what you are trying to predict. If you are concerned with predicting only points inside the $[0,1]$ interval, then yes, these models seem approximate well points that were not included in the original sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label='Samples')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, square_model.predict(X_poly_test), color='g',label='Square Fit')\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", square_model.score(X_poly_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label='Target')\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, Y_hat, color='g',label=\"MLP\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", mlp_model.score(X_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But here's what our models looks like when we extend the range of the plot a bit to show points farther away from the sample we used to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = np.linspace(-1,2,100).reshape(-1,1)\n",
    "\n",
    "new_Y = f(new_X)\n",
    "\n",
    "new_X_poly = poly.fit_transform(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_X,new_Y,label=\"Target\")\n",
    "\n",
    "plt.plot(new_X, linear_model.predict(new_X), color='r',label=\"Linear Fit\")\n",
    "plt.plot(new_X, square_model.predict(new_X_poly), color='g',label=\"Square Fit\")\n",
    "plt.plot(new_X, mlp_model.predict(new_X), color='y',label=\"MLP Fit\")\n",
    "\n",
    "plt.legend(loc=\"upper right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the in-sample error, or **Training Error**, is not a very good measure of how well the model will do on out-of-sample data. In fact, models can have an arbitrarily small **Training Error** and be very bad at predicting out-of-sample values. To see this, consider what would happen if you were to try the example above with a very wiggly function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LET'S TRY FITTING A DEGREE 100 POLYNOMIAL TO OUR SAMPLES\n",
    "\n",
    "poly_too_high = PolynomialFeatures(degree=100)\n",
    "\n",
    "too_high_model = LinearRegression()\n",
    "\n",
    "X_poly = poly_too_high.fit_transform(X)  #TRANSFORM X INTO X^n\n",
    "\n",
    "too_high_model.fit(X_poly,Y)\n",
    "\n",
    "X_poly_test = poly_too_high.fit_transform(X_test)\n",
    "\n",
    "plt.scatter(X,Y,label=\"Samples\")\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, too_high_model.predict(X_poly_test), color='g',label=\"Degree 100 Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"In-sample Performance:\", too_high_model.score(X_poly,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test, f(X_test), color='r',label=\"Target\")\n",
    "plt.scatter(X,Y,label=\"Samples\")\n",
    "plt.ylim((-2,2))\n",
    "plt.plot(X_test, too_high_model.predict(X_poly_test), color='g',label=\"Degree 100 Fit\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "print(\"Out-of-sample Performance:\", too_high_model.score(X_poly_test,f(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AND THEN ZOOMING OUT\n",
    "\n",
    "new_X_too_high = poly_too_high.fit_transform(new_X)\n",
    "\n",
    "plt.scatter(new_X,new_Y,label=\"Target\")\n",
    "plt.plot(new_X, too_high_model.predict(new_X_too_high), color='r',label=\"Linear Fit\")\n",
    "plt.ylim((-5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the ***training*** error here is low - the model actually passes right through most of the points in the sample! But it is also obvious that this model is not very accurate at predicting points outside the sample, whether that's close to the range of the sample or not. \n",
    "\n",
    "So in practice you will need to have not one, but TWO datasets. One to train your model on and another to validate it. This second dataset is called a **Test Set** and it is a collection of inputs and outputs you obtained from the same source as your training data, but that you **did not use to train your model.**\n",
    "\n",
    "It is the error in this dataset, called the **Test Error** that matters when we talk about the quality of our predictions, and hence the quality of our model.\n",
    "\n",
    "Concretely, you will train your model on the **Training Set**, looking to minimize the **Training Error**. Then you will pick up the model you trained and plug in the inputs from your **Test Set**. You will then use the outputs you get from your model and compare them to the outputs in your **Test Set**. This will allow you to compute your **Test Error**.\n",
    "\n",
    "Rinse and repeat until you're statisfied with the performance of your model on the **Test Set**!\n",
    "\n",
    "A common rule-of-thumb is to break your initial data set in two chunks: about 80% of all examples go into your **Training Set**, the remaining 20% are set aside for your **Test Set**.\n",
    "\n",
    "Another popular approach is to break your initial data set in THREE chunks: a **Training Set**, a **Validation Set** and a **Test Set**, where the **Validation Set** is not used directly to train the model, but the error in this set is used as the yardstick to *fine-tune* a model, so it participates indirectly in the training.\n",
    "\n",
    "\n",
    "Let's look into some examples with real datasets to see how this works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example 1: Flower Species Classification\n",
    "\n",
    "In the toy examples we've seen above, we used a mathematical function to generate points and used different algorithms to try and approximate it. In that case, both our input X and output Y were quantitative (a real number representing a quantity). In Machine Learning this type of task, i.e. predicting a quantitative output, is called **Regression**.\n",
    "\n",
    "Now let's look at an example where the output is no longer quantitative, but *categorical*, meaning that the output variable represents categories of things - a task called **Classification**.\n",
    "\n",
    "We will use a popular Python Machine Learning library called **scikit-learn** to show how it works in practice.\n",
    "\n",
    "### The Iris dataset\n",
    "\n",
    "This example uses a classic dataset called the \"Iris\" dataset. It contains a number of measurements of different species of Iris flowers along with a label indicating which of 3 species of Iris the measurements came from.\n",
    "\n",
    "We will train a model to take in measurements as inputs and predict the species.\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "headers = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species']\n",
    "\n",
    "iris_dataset = read_csv('./data/iris.csv', names = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset) # A GOOD FIRST STEP IS TO LOOK AT THE ACTUAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.describe()) # THEN COMPUTE SUMMARY STATISTICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris_dataset.groupby('species').size()) # ARE THERE IMBALANCES IN THE OUTPUT CATEGORIES?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "scatter_matrix(iris_dataset) # PLOT ALL VARIABLES 2 BY 2... ARE THERE ANY VISIBLE PATTERNS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S CREATE A TRAINING SET AND A TEST SET\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = iris_dataset.values[:,0:4]\n",
    "Y = iris_dataset.values[:,4]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S TRY DIFFERENT ALGORITHMS - FIRST A LINEAR MODEL: LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, plot_confusion_matrix\n",
    "\n",
    "lr_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "lr_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = lr_model.predict(X_test)\n",
    "\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(lr_model,X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOTHER MODEL: TREE CLASSIFIERS\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "tree_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = tree_model.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(tree_model,X_test, Y_test)\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE MORE: SUPPORT VECTOR MACHINES (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel = 'linear')\n",
    "\n",
    "svm_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = svm_model.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(svm_model,X_test, Y_test)\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more options of Algorithms, see: https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Wine Classification\n",
    "\n",
    "Now it's your turn. This next dataset contains a number of measurements done in a chemical analysis of 3 different types of wine.\n",
    "\n",
    "You will train a Support Vector Machine model to predict the type of wine based on the measurements. Use the example above as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "headers = ['wine_type','alcohol', 'malic_acid','ash','alcalinity_of_ash','magnesium',\n",
    "           'total_phenols','flavanoids','nonflavanoid_phenols','proanthocyanins','color_intensity','hue','OD280_OD315','proline']\n",
    "\n",
    "wine_dataset = read_csv('./data/wine.csv', names = headers)\n",
    "\n",
    "# PRINT SUMARY STATISTICS\n",
    "\n",
    "# PRINT THE DATASET\n",
    "\n",
    "# LOOK FOR IMBALANCES - THE OUPUT VARIABLE IS 'wine_type'\n",
    "\n",
    "# LOOK FOR VISUAL PATTERNS\n",
    "\n",
    "# CREATE TRAINING AND TEST SETS - THE OUTPUT VARIBALE 'wine_type' IS ON COLUMN 0\n",
    "\n",
    "# FIT A MODEL TO THE TRAINING SET\n",
    "\n",
    "# CHECK ITS PERFORMANCE ON THE TEST SET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Example 2: Image Classification\n",
    "\n",
    "In the Flower Species Classification example we had a dataset with numerical features/inputs and a categorical response/output. The categories in our example appeared as text, but the code transformed them into numbers under the hood. What other types of data can be encoded as numbers?\n",
    "\n",
    "The answer is: pretty much anything can. \n",
    "\n",
    "Let's look at a Classification problem using images. We will train a model to classify images as either Apples or Oranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "an_apple = Image.open('./Fruit-Images-Dataset/Training/apples/0_100.jpg')\n",
    "\n",
    "print(\"This is how the computer 'sees' the image: \\n\")\n",
    "\n",
    "print(np.array(an_apple)[:,:,0], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.array(an_apple)[:,:,:]) # TRY PRINTING THE THREE DIFFERENT CHANNELS - [:,:,0], [:,:,1], [:,:,2]\n",
    "print(\"This is how we see it:\"):,:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_orange = Image.open('./Fruit-Images-Dataset/Training/oranges/0_100.jpg')\n",
    "\n",
    "plt.imshow(an_orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET'S CREATE OUR TRAINING SET!\n",
    "\n",
    "apples_dir = './Fruit-Images-Dataset/Training/apples/'\n",
    "oranges_dir = './Fruit-Images-Dataset/Training/oranges/'\n",
    "\n",
    "#os.listdir(apples_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD APPLES STEP BY STEP - WITH ILLUSTRATIVE VARIABLE NAMES\n",
    "\n",
    "apples = []\n",
    "\n",
    "for filename in os.listdir(apples_dir):\n",
    "    \n",
    "    apple_full_path = apples_dir + filename\n",
    "    \n",
    "    apple = Image.open(apple_full_path)\n",
    "    \n",
    "    apple_flattened = np.array(apple).flatten()\n",
    "    \n",
    "    apple_resized = apple_flattened/255\n",
    "    \n",
    "    a_training_example = (apple_resized, \"apple\")\n",
    "    \n",
    "    apples.append(a_training_example)\n",
    "\n",
    "apples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ORANGES WITH A PYTHON ONE-LINER... CAN YOU SEE HOW THIS DOES THE SAME THING AS THE FOR-LOOP BLOCK ABOVE?\n",
    "\n",
    "oranges = [ (np.array(Image.open(oranges_dir + img)).flatten()/255,\"orange\") for img in os.listdir(oranges_dir) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZIP SEPARATES ARRAYS FROM LABELS\n",
    "\n",
    "X_train,Y_train = zip(*(apples + oranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE'S HOW ZIP WORKS:\n",
    "\n",
    "my_list = [(np.array([1,2,3]), \"one\"),(np.array([4,5,6]), \"two\")]\n",
    "\n",
    "a,b = zip(*my_list)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACK \"STACKS\" VECTORS INTO A MATRIX\n",
    "\n",
    "X_train = np.stack(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE'S HOW STACK WORKS\n",
    "\n",
    "print(a)\n",
    "\n",
    "np.stack(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW THE TEST SET\n",
    "\n",
    "apples_dir = './Fruit-Images-Dataset/Test/apples/'\n",
    "oranges_dir = './Fruit-Images-Dataset/Test/oranges/'\n",
    "\n",
    "apples = [ (np.array(Image.open(apples_dir + img)).flatten()/255,\"apple\") for img in os.listdir(apples_dir) ]\n",
    "\n",
    "oranges = [ (np.array(Image.open(oranges_dir + img)).flatten()/255,\"orange\") for img in os.listdir(oranges_dir) ]\n",
    "\n",
    "X_test,Y_test = zip(*(apples + oranges))\n",
    "\n",
    "X_test = np.stack(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THEN TRAIN A LOGISTIC REGRESSION MODEL AND SEE HOW IT DOES\n",
    "\n",
    "lr_model = LogisticRegression(solver='liblinear', multi_class='ovr')\n",
    "\n",
    "lr_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_hat = lr_model.predict(X_test)\n",
    "\n",
    "plot_confusion_matrix(lr_model,X_test, Y_test)\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Handwritten Digit Classification\n",
    "\n",
    "Your turn again. This next dataset is another classic: the MNIST dataset of handwritten digits.\n",
    "\n",
    "In this exercise, you will train a model of your choice on images of handwritten numbers that sometimes look alike when people write them: 0, 6 and 8.\n",
    "\n",
    "Use all the examples we've seen so far as insipration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START BY CREATING YOUR TRAINING SET...\n",
    "\n",
    "zero_dir = './MNIST-Dataset/Training/0/'\n",
    "six_dir = './MNIST-Dataset/Training/6/'\n",
    "eight_dir = './MNIST-Dataset/Training/8/'\n",
    "\n",
    "\n",
    "X_train,Y_train = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...AND YOUR TEST SET\n",
    "\n",
    "zero_dir = './MNIST-Dataset/Test/0/'\n",
    "six_dir = './MNIST-Dataset/Test/6/'\n",
    "eight_dir = './MNIST-Dataset/Test/8/'\n",
    "\n",
    "X_test, Y_test = ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN YOUR MODEL AND EVALUATE ITS PERFORMANCE ON THE TEST SET\n",
    "\n",
    "my_model = ####\n",
    "\n",
    "Y_hat = my_model(####)\n",
    "\n",
    "plot_confusion_matrix(my_model,X_test, Y_test)\n",
    "\n",
    "print(\"This model got\", accuracy_score(Y_test, Y_hat)*100, \"% of predictions right.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing what we've seen so far\n",
    "\n",
    "Based on what we've covered so far, a Machine Learning program will generally have the following elements:\n",
    "\n",
    "1. A class of functions $f(x;\\theta)$ that the algorithm will use to try and approximate the taget $F^*$. \n",
    "\n",
    "    a. $f(x;\\theta)$ can be a restrictive class of functions such as linear functions ($\\beta_0 + \\sum{\\beta_i X_i}$) or a very flexible one, such as Neural Networks (we will see what they look like on the next notebook)\n",
    "    \n",
    "2. A Training Set: a dataset containing examples of inputs and outputs of interest.  \n",
    "\n",
    "3. A Test Set: another dataset with examples of inputs and outputs that are not used to train the model.\n",
    "\n",
    "4. A measure of the error in using $f(x;\\theta)$ to approximate $F^*$, called the Loss function $L(Y,f(X;\\theta))$. $L$ is used to train the model on the training set (see point 5) and can be used to measure performance on the Test Set. \n",
    "\n",
    "5. A way of solving the optimization problem: $\\hat{F(X)} = \\underset{\\theta}{\\operatorname{argmin}} L(Y,f(X;\\theta))$\n",
    "\n",
    "In the **scikit-learn** examples above, you will notice that 4, 5 and certain aspects of 1 are done mostly under the hood, leaving very little control up to you.\n",
    "\n",
    "Next we turn to a more modern Machine Learning library that is better suited for high performance and solving difficult problems: **PyTorch**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
